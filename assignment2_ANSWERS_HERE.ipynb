{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a972d6d",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "\n",
    "Use Multiple Linear Regression for this task. Display the coefficients of the model and calculate the MAE (Mean Absolute Error) and MSE (Mean Squared Error). Search about RMSE (Root Mean Squared Error) and explain the trade-offs between these metrics. Finally report RMSE score of your model.\n",
    "Perform this task using both LinearRegression and SGDRegressor.\n",
    "Additionally, study the MAPE (Mean Absolute Percentage Error) metric using this link, and apply it to evaluate your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab28d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR REGRESSION\n",
      "----------------------------------------\n",
      "Coefficients:\n",
      "          Coefficient\n",
      "size       143.218532\n",
      "bedroom -13512.564426\n",
      "Intercept: 84763.62\n",
      "\n",
      "Metrics:\n",
      "MAE:  72334.75\n",
      "MSE:  8610424544.78\n",
      "RMSE: 92792.37\n",
      "MAPE: 0.1746 (17.46%)\n",
      "\n",
      "\n",
      "SGD REGRESSOR\n",
      "----------------------------------------\n",
      "Coefficients (scaled features):\n",
      "           Coefficient\n",
      "size     106535.910237\n",
      "bedroom  -10274.951289\n",
      "Intercept: 323155.83\n",
      "\n",
      "Metrics:\n",
      "MAE:  72124.61\n",
      "MSE:  8595003325.39\n",
      "RMSE: 92709.24\n",
      "MAPE: 0.1740 (17.40%)\n",
      "\n",
      "\n",
      "MODEL COMPARISON\n",
      "----------------------------------------\n",
      "  Metric  LinearRegression  SGDRegressor\n",
      "0    MAE      7.233475e+04  7.212461e+04\n",
      "1    MSE      8.610425e+09  8.595003e+09\n",
      "2   RMSE      9.279237e+04  9.270924e+04\n",
      "3   MAPE      1.746000e-01  1.740000e-01\n",
      "\n",
      "\n",
      "METRICS TRADE-OFFS\n",
      "----------------------------------------\n",
      "MAE  - Average absolute error, robust to outliers.\n",
      "MSE  - Penalizes large errors more due to squaring.\n",
      "RMSE - Same units as target; balances MAE and MSE.\n",
      "MAPE - Scale-independent; expresses error in %.\n",
      "\n",
      "Note: RMSE is often preferred because it:\n",
      "- Uses the same unit as the target variable\n",
      "- Penalizes larger errors more than MAE\n",
      "- Is more interpretable than MSE in most cases\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Load and Prepare Data\n",
    "# ===============================\n",
    "\n",
    "# Load housing dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\rtape\\Downloads\\Seneca\\CVI620NSB_Summer2025\\codes\\Assignment2\\Q1\\house_price.csv\")\n",
    "\n",
    "# Select features (size, bedroom) and target (price)\n",
    "X = df[['size', 'bedroom']]\n",
    "y = df['price']\n",
    "\n",
    "# Split into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Linear Regression Model\n",
    "# ===============================\n",
    "\n",
    "print(\"LINEAR REGRESSION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Initialize and train Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Display learned coefficients\n",
    "coeff_df = pd.DataFrame(lr_model.coef_, X.columns, columns=['Coefficient'])\n",
    "print(\"Coefficients:\")\n",
    "print(coeff_df)\n",
    "print(f\"Intercept: {lr_model.intercept_:.2f}\")\n",
    "\n",
    "# Predict on test data and evaluate performance\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_mape = mean_absolute_percentage_error(y_test, y_pred_lr)\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"MAE:  {lr_mae:.2f}\")\n",
    "print(f\"MSE:  {lr_mse:.2f}\")\n",
    "print(f\"RMSE: {lr_rmse:.2f}\")\n",
    "print(f\"MAPE: {lr_mape:.4f} ({lr_mape*100:.2f}%)\")\n",
    "\n",
    "# ===============================\n",
    "# Step 3: SGD Regressor Model\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\\nSGD REGRESSOR\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Standardize features (required for gradient-based models like SGD)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train SGD Regressor\n",
    "sgd_model = SGDRegressor(max_iter=1000, random_state=42)\n",
    "sgd_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Display learned coefficients (note: scaled features)\n",
    "sgd_coeff_df = pd.DataFrame(sgd_model.coef_, X.columns, columns=['Coefficient'])\n",
    "print(\"Coefficients (scaled features):\")\n",
    "print(sgd_coeff_df)\n",
    "print(f\"Intercept: {sgd_model.intercept_[0]:.2f}\")\n",
    "\n",
    "# Predict and evaluate performance\n",
    "y_pred_sgd = sgd_model.predict(X_test_scaled)\n",
    "sgd_mae = mean_absolute_error(y_test, y_pred_sgd)\n",
    "sgd_mse = mean_squared_error(y_test, y_pred_sgd)\n",
    "sgd_rmse = np.sqrt(sgd_mse)\n",
    "sgd_mape = mean_absolute_percentage_error(y_test, y_pred_sgd)\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "print(f\"MAE:  {sgd_mae:.2f}\")\n",
    "print(f\"MSE:  {sgd_mse:.2f}\")\n",
    "print(f\"RMSE: {sgd_rmse:.2f}\")\n",
    "print(f\"MAPE: {sgd_mape:.4f} ({sgd_mape*100:.2f}%)\")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Model Comparison\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\\nMODEL COMPARISON\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a comparison table of evaluation metrics\n",
    "comparison = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'MSE', 'RMSE', 'MAPE'],\n",
    "    'LinearRegression': [lr_mae, lr_mse, lr_rmse, lr_mape],\n",
    "    'SGDRegressor': [sgd_mae, sgd_mse, sgd_rmse, sgd_mape]\n",
    "})\n",
    "print(comparison.round(4))\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Metrics Explanation\n",
    "# ===============================\n",
    "\n",
    "print(\"\\n\\nMETRICS TRADE-OFFS\")\n",
    "print(\"-\" * 40)\n",
    "print(\"MAE  - Average absolute error, robust to outliers.\")\n",
    "print(\"MSE  - Penalizes large errors more due to squaring.\")\n",
    "print(\"RMSE - Same units as target; balances MAE and MSE.\")\n",
    "print(\"MAPE - Scale-independent; expresses error in %.\")\n",
    "\n",
    "print(f\"\\nNote: RMSE is often preferred because it:\")\n",
    "print(\"- Uses the same unit as the target variable\")\n",
    "print(\"- Penalizes larger errors more than MAE\")\n",
    "print(\"- Is more interpretable than MSE in most cases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731ab48",
   "metadata": {},
   "source": [
    "### QUESTION 2\n",
    "\n",
    "For the Cat and Dog dataset provided in the Q2 folder, perform classification using all the methods you know and try to achieve the best possible result. Compare the algorithms carefully and tune the parameters so that the best result can be obtained.\n",
    "Save the trained model and test it on several images from the internet. Was the model able to correctly predict the images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7250a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:\n",
      "KNN: 0.5556\n",
      "Logistic Regression: 0.5920\n",
      "SGD: 0.5755\n",
      "\n",
      "Best Model: Logistic Regression (0.5920)\n",
      "Model saved as best_cat_dog_model.pkl\n",
      "\n",
      "Test Results:\n",
      "Cat (1).jpg: Cat (83.2%) (Actual: Cat)\n",
      "Dog (1).jpg: Dog (77.8%) (Actual: Dog)\n",
      "\n",
      "To test internet images: test_image('path_to_image.jpg')\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Path to the dataset directory\n",
    "base_path = r\"C:\\Users\\rtape\\Downloads\\Seneca\\CVI620NSB_Summer2025\\codes\\Assignment2\\Q2\"\n",
    "\n",
    "# -------------------\n",
    "# Step 1: Load and preprocess data\n",
    "# -------------------\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through all image files in the dataset\n",
    "for address in glob.glob(os.path.join(base_path, '*', '*', '*')):\n",
    "    img = cv2.imread(address)  # Read image\n",
    "    if img is None:\n",
    "        continue  # Skip if image couldn't be loaded\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32))  # Resize to 32x32 pixels\n",
    "    img = img.flatten() / 255.0      # Flatten to 1D array and normalize pixel values\n",
    "    data.append(img)\n",
    "    \n",
    "    # Assign label based on folder name: 0 = Cat, 1 = Dog\n",
    "    labels.append(0 if 'Cat' in address else 1)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(data)\n",
    "y = np.array(labels)\n",
    "\n",
    "# -------------------\n",
    "# Step 2: Split and scale data\n",
    "# -------------------\n",
    "\n",
    "# Split dataset into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features to have 0 mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# -------------------\n",
    "# Step 3: Train models using GridSearchCV\n",
    "# -------------------\n",
    "\n",
    "models = {}\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=3)\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "models['KNN'] = (knn_grid.best_estimator_, accuracy_score(y_test, knn_grid.predict(X_test_scaled)))\n",
    "\n",
    "# Logistic Regression\n",
    "lr_params = {'C': [0.1, 1, 10], 'solver': ['lbfgs', 'liblinear']}\n",
    "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000), lr_params, cv=3)\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "models['Logistic Regression'] = (lr_grid.best_estimator_, accuracy_score(y_test, lr_grid.predict(X_test_scaled)))\n",
    "\n",
    "# SGD Classifier (Stochastic Gradient Descent)\n",
    "sgd_params = {'alpha': [0.0001, 0.001, 0.01], 'loss': ['hinge', 'log_loss']}\n",
    "sgd_grid = GridSearchCV(SGDClassifier(max_iter=1000), sgd_params, cv=3)\n",
    "sgd_grid.fit(X_train_scaled, y_train)\n",
    "models['SGD'] = (sgd_grid.best_estimator_, accuracy_score(y_test, sgd_grid.predict(X_test_scaled)))\n",
    "\n",
    "# -------------------\n",
    "# Step 4: Compare model performances\n",
    "# -------------------\n",
    "\n",
    "print(\"RESULTS:\")\n",
    "for name, (model, acc) in models.items():\n",
    "    print(f\"{name}: {acc:.4f}\")  # Print accuracy of each model\n",
    "\n",
    "# -------------------\n",
    "# Step 5: Save the best model\n",
    "# -------------------\n",
    "\n",
    "# Identify the best performing model\n",
    "best_name = max(models.keys(), key=lambda x: models[x][1])\n",
    "best_model, best_acc = models[best_name]\n",
    "\n",
    "# Save best model and scaler to disk\n",
    "joblib.dump(best_model, 'best_cat_dog_model.pkl')\n",
    "joblib.dump(scaler, 'best_cat_dog_scaler.pkl')\n",
    "\n",
    "print(f\"\\nBest Model: {best_name} ({best_acc:.4f})\")\n",
    "print(\"Model saved as best_cat_dog_model.pkl\")\n",
    "\n",
    "# -------------------\n",
    "# Step 6: Test new image function\n",
    "# -------------------\n",
    "\n",
    "# Predict label for new image\n",
    "def test_image(image_path):\n",
    "    # Load saved model and scaler\n",
    "    model = joblib.load('best_cat_dog_model.pkl')\n",
    "    scaler = joblib.load('best_cat_dog_scaler.pkl')\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return \"Could not load image\"\n",
    "    \n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    img = img.flatten() / 255.0\n",
    "    img_scaled = scaler.transform(img.reshape(1, -1))\n",
    "    \n",
    "    # Predict label (0 = Cat, 1 = Dog)\n",
    "    prediction = model.predict(img_scaled)[0]\n",
    "    label = \"Cat\" if prediction == 0 else \"Dog\"\n",
    "    \n",
    "    # If model supports confidence scoring\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        confidence = max(model.predict_proba(img_scaled)[0]) * 100\n",
    "        return f\"{label} ({confidence:.1f}%)\"\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "# -------------------\n",
    "# Step 7: Test on sample dataset images\n",
    "# -------------------\n",
    "\n",
    "# List of test images to try\n",
    "test_paths = [\n",
    "    os.path.join(base_path, \"test\", \"Cat\", \"Cat (1).jpg\"),\n",
    "    os.path.join(base_path, \"test\", \"Dog\", \"Dog (1).jpg\")\n",
    "]\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "for path in test_paths:\n",
    "    if os.path.exists(path):\n",
    "        result = test_image(path)\n",
    "        actual = \"Cat\" if \"Cat\" in path else \"Dog\"\n",
    "        print(f\"{os.path.basename(path)}: {result} (Actual: {actual})\")\n",
    "\n",
    "# -------------------\n",
    "# Final Note\n",
    "# -------------------\n",
    "\n",
    "print(\"\\nTo test internet images: test_image('path_to_image.jpg')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b8416",
   "metadata": {},
   "source": [
    "### QUESTION 3\n",
    "\n",
    "The MNIST dataset is one of the most well-known datasets in the field of image processing. It contains 60,000 images related to handwritten digits from 0 to 9 and is provided as a CSV file in the Q3 folder. In this file, each image is represented as a flattened vector. Classify this dataset using different methods and try to achieve at least 90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd866ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (59999, 784), Test shape: (9999, 784)\n",
      "\n",
      "KNN CLASSIFIER\n",
      "------------------------------\n",
      "Accuracy: 0.9452\n",
      "\n",
      "LOGISTIC REGRESSION\n",
      "------------------------------\n",
      "Accuracy: 0.9219\n",
      "\n",
      "MODEL COMPARISON\n",
      "------------------------------\n",
      "KNN Accuracy:              0.9452\n",
      "Logistic Regression Accuracy: 0.9219\n",
      "✓ Target Achieved (≥ 90%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Load and Prepare the Data\n",
    "# ===============================\n",
    "\n",
    "# Load MNIST training and testing data from CSV files\n",
    "train_data = pd.read_csv(r\"C:\\Users\\rtape\\Downloads\\Seneca\\CVI620NSB_Summer2025\\codes\\Assignment2\\Q3\\mnist_train.csv\")\n",
    "test_data = pd.read_csv(r\"C:\\Users\\rtape\\Downloads\\Seneca\\CVI620NSB_Summer2025\\codes\\Assignment2\\Q3\\mnist_test.csv\")\n",
    "\n",
    "# Split into features (X) and labels (y)\n",
    "X_train = train_data.iloc[:, 1:].values / 255.0  # Normalize pixel values to [0, 1]\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "\n",
    "X_test = test_data.iloc[:, 1:].values / 255.0\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Feature Scaling\n",
    "# ===============================\n",
    "\n",
    "# Standardize features: mean = 0, std = 1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ===============================\n",
    "# Step 3: KNN Classifier\n",
    "# ===============================\n",
    "\n",
    "print(\"\\nKNN CLASSIFIER\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Initialize and train K-Nearest Neighbors (k=3)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test data and evaluate accuracy\n",
    "knn_pred = knn.predict(X_test_scaled)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "print(f\"Accuracy: {knn_accuracy:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Logistic Regression\n",
    "# ===============================\n",
    "\n",
    "print(\"\\nLOGISTIC REGRESSION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Initialize and train Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test data and evaluate accuracy\n",
    "lr_pred = lr.predict(X_test_scaled)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Model Comparison\n",
    "# ===============================\n",
    "\n",
    "print(\"\\nMODEL COMPARISON\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"KNN Accuracy:              {knn_accuracy:.4f}\")\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
    "\n",
    "# Check if either model achieved ≥ 90% accuracy\n",
    "if max(knn_accuracy, lr_accuracy) >= 0.90:\n",
    "    print(\"✓ Target Achieved (≥ 90%)\")\n",
    "else:\n",
    "    print(\"✗ Target Not Met (< 90%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python CVI620NSB",
   "language": "python",
   "name": "cvi620nsb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
